<!DOCTYPE html>
<html>
<head>
<title>readme.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="nerf%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87">NERF相关论文</h1>
<h2 id="1-nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesispapercode">1. NERF: Representing Scenes as Neural Radiance Fields for View Synthesis<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EfjwUxckpWpMu0nAZaVbMd4BblZ1lH8KMTQv7M0-UAu7Cw?e=ypdkyX">[paper]</a><a href="https://github.com/bmild/nerf">[code]</a></h2>
<p> 这篇论文属于nerf的开篇之作，非常具有代表性，非常适合初学者仔细阅读，在网上也可以找到这篇论文的解读，非常详细，比如这篇<a href="https://zhuanlan.zhihu.com/p/481275794">[解读]</a>。在这个作者的后续文章中甚至有自己完成的简化版colab代码，非常值得学习。</p>
<h2 id="2-plenoxels-radiance-fields-without-neural-networkspapercode">2. Plenoxels: Radiance Fields without Neural Networks<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EVRC9_EN-8NKgHC7oY8IDcoBa3Mu59t4tyUjBP4OJAewLA?e=93FQXk">[paper]</a><a href="https://github.com/sxyu/svox2">[code]</a></h2>
<p> 这篇论文对nerf的训练速度进行了很好的优化，主要是因为去掉了nerf中最耗时的步骤-神经网络，应用上了更合理的辐射场表示模型，再加上将平滑的任务交给了插值而不是神经网络大大提高了神经网络的训练速度，非常具有参考价值（也许可以应用到其他对nerf进行扩展的模型方法上，进行进一步的改进）。</p>
<h1 id="nerf%E4%B8%8Egan%E7%BB%93%E5%90%88%E8%BF%9B%E8%A1%8C%E5%BF%AB%E9%80%9F%E7%9A%843d%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90">NERF与GAN结合，进行快速的3d模型生成</h1>
<h2 id="1-graf-generative-radiance-fields-for-3d-shape-synthesispapercode">1. GRAF: Generative Radiance Fields for 3D Shape Synthesis<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EXSNtjW1Aa1GueB9klUVoPkBSI22GRkk2w8l3MCGpSetLg?e=as9ual">[paper]</a><a href="https://github.com/autonomousvision/graf">[code]</a></h2>
<p> 这篇论文是nerf与GAN结合的开篇之作，将nerf的辐射场表示模型与GAN结合，可以快速的生成3d模型，我个人认为这是一种快速生成具有先验信息的模型的类模型的神经辐射场的一种快速超级加速方案。不过可能是因为成绩不够惊艳，网上对这篇论文的解读并不多，但是这篇论文的思路是非常值得学习的，可以参考这篇<a href="https://zhuanlan.zhihu.com/p/388136772">[解读]</a>。</p>
<h2 id="2-giraffe-representing-scenes-as-compositional-generative-neural-feature-fieldspapercode">2. GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EXlQ2KnxozhJrtZUKWXVDNUB_m0TdprOCT6wa9ZPec5O5A?e=GE5WuQ">[paper]</a><a href="https://github.com/autonomousvision/giraffe">[code]</a></h2>
<p> 这篇论文的作者和GRAF好像是出自一个团队，属于是对GRAF的进一步改进，主要引入了辐射场transform和拼接，实现了将多个三维渲染模型放入同一个场景下的需求，根据对隐码的进一步分析与操作，甚至可以实现随意改变三维物体的特定特征，比如颜色，材质等等，这个解读也有提到<a href="https://zhuanlan.zhihu.com/p/388136772">[解读]</a>。</p>
<h2 id="3-giraffe-hd-a-high-resolution-3daware-generative-modelpaper">3. GIRAFFE HD A High-Resolution 3D_aware Generative Model<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EdmB-IdESIdHv7fXlCaTpmcBEDcFKEOSf5wNIRvAGoConQ?e=CwMurx">[paper]</a></h2>
<p> 这篇论文是2022年发表的，在网上很难找到相关的内容，复现起来应该很有难度，论文主要是对GIRAFFE模型生成的图像进行了更加复杂的后续处理，得到了分辨率和细节都更加高的图像。</p>
<h1 id="%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E5%8C%96">人体模型参数化</h1>
<h2 id="smpl">SMPL</h2>
<p> SMPL属于非常经典的人体模型参数化方案了，网上也有很多教程，实际上就是把制作人体模型的美术流程数学化的一个模型，基于MESH表达和蒙皮转化，涉及到大量的美工术语，具体教程可以看这里<a href="https://zhuanlan.zhihu.com/p/256358005">[教程]</a>。</p>
<h2 id="3dmm">3DMM</h2>
<p>  3DMM，即三维可变形人脸模型，是一个通用的三维人脸模型，用固定的点数来表示人脸。**它的核心思想就是人脸可以在三维空间中进行一一匹配，并且可以由其他许多幅人脸正交基加权线性相加而来。**我们所处的三维空间，每一点(x,y,z)，实际上都是由三维空间三个方向的基量，(1,0,0)，(0,1,0)，(0,0,1)加权相加所得，只是权重分别为x,y,z。</p>
<p>  转换到三维空间，道理也一样。每一个三维的人脸，可以由一个数据库中的所有人脸组成的基向量空间中进行表示，而求解任意三维人脸的模型，实际上等价于求解各个基向量的系数的问题。</p>
<p>  人脸的基本属性包括形状和纹理，每一张人脸可以表示为形状向量和纹理向量的线性叠加。</p>
<p>  形状向量Shape Vector：S=(X1,Y1,Z1,X2,Y2,Z2,...,Yn,Zn)，示意图如下：</p>
<p><img src="https://pic3.zhimg.com/v2-58bb650a63e71c194ff00469753cf6b6_b.jpg" alt=""></p>
<p><img src="https://pic3.zhimg.com/80/v2-58bb650a63e71c194ff00469753cf6b6_1440w.webp" alt=""></p>
<p>  纹理向量Texture Vector：T=(R1,G1,B1,R2,G2,B2,...,Rn,Bn)，示意图如下：</p>
<p><img src="https://pic4.zhimg.com/v2-6a90450d84d1fb00cc7891c042fad17f_b.jpg" alt=""></p>
<p><img src="https://pic4.zhimg.com/80/v2-6a90450d84d1fb00cc7891c042fad17f_1440w.webp" alt=""></p>
<p>  任意的人脸模型可以由数据集中的m个人脸模型进行加权组合如下：</p>
<p><img src="https://pic2.zhimg.com/v2-fa8dc46ca1dab4735098c7eff0adf1b5_b.jpg" alt=""></p>
<p><img src="https://pic2.zhimg.com/80/v2-fa8dc46ca1dab4735098c7eff0adf1b5_1440w.webp" alt=""></p>
<p>  其中Si，Ti就是数据库中的第i张人脸的形状向量和纹理向量。但是我们实际在构建模型的时候不能使用这里的Si，Ti作为基向量，因为它们之间不是正交相关的，所以接下来需要使用PCA进行降维分解。</p>
<p>(1) 首先计算形状和纹理向量的平均值。</p>
<p>(2) 中心化人脸数据。</p>
<p>(3) 分别计算协方差矩阵。</p>
<p>(4) 求得形状和纹理协方差矩阵的特征值α，β和特征向量si，ti。</p>
<p>  上式可以转换为下式</p>
<p><img src="https://pic1.zhimg.com/v2-b56050beadc09585a6a0ff49b1ddef30_b.jpg" alt=""></p>
<p><img src="https://pic1.zhimg.com/80/v2-b56050beadc09585a6a0ff49b1ddef30_1440w.webp" alt=""></p>
<p>  其中第一项是形状和纹理的平均值，而si，ti则都是Si，Ti减去各自平均值后的协方差矩阵的特征向量，它们对应的特征值按照大小进行降序排列。</p>
<p>  等式右边仍然是m项，但是累加项降了一维，减少了一项。si，ti都是线性无关的，取其前几个分量可以对原始样本做很好的近似，因此可以大大减少需要估计的参数数目，并不失精度。</p>
<p>  基于3DMM的方法，都是在求解这几个系数，随后的很多模型会在这个基础上添加表情，光照等系数，但是原理与之类似。</p>
<h2 id="mano%E6%89%8B%E9%83%A8%E6%A8%A1%E5%9E%8B">MANO手部模型</h2>
<p>  类似于SMPL的一种手部模型表示方案，生成的模型的精确度取决于皮肤顶点（vertices）的数量，后来的改进版本MANO-2则使用了更多的顶点，从而提高了模型的精确度。类似于<a href="https://github.com/dkulon/hand-reconstruction">这个仓库</a>,<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EVTFUZo-mx5FhwzaXfnSbK0BrWMtTlJazUehP1U8HawyJg?e=5ow882">对应论文</a>。使用这里的可视化手部模型生成器生成大量手部模型样本，用于后续模型验证。</p>
<h1 id="%E5%9F%BA%E4%BA%8Enerf%E5%92%8C%E5%8F%82%E6%95%B0%E5%8C%96%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA">基于nerf和参数化人体模型的人体重建</h1>
<h2 id="neural-body-implicit-neural-representations-with-structured-latent-codes-for-novel-view-synthesis-of-dynamic-humanspapercode">Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EWr4NkjmXMlFjPadF2E-ohoBb-ddMf6uEdVSwB6IFQ5nkA?e=h3mWvD">[paper]</a><a href="https://github.com/zju3dv/neuralbody">[code]</a></h2>
<p>将变形场与NERF进行了很好的结合，在降低了NERF对数据要求过于严格的基础上，也提高了模型生成效果，更是使用变形场使得模型能够动起来，实现动态重建的效果。</p>
<h2 id="animatable-neural-radiance-fields-for-modeling-dynamic-human-bodiespapercode">Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies<a href="https://mailscuteducn-my.sharepoint.com/:b:/g/personal/201966176253_mail_scut_edu_cn/EURnyv4lqq5Lk4ydXLjvBzsBQa15DSqJxBcDBof7jZtDqw?e=D3x2Bg">[paper]</a><a href="https://github.com/zju3dv/animatable_nerf">[code]</a></h2>
<p>在neural bady的基础上，引入了参数化模型来表示人体，实现了更加优秀的人体重建效果。</p>

</body>
</html>
